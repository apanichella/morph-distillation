Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
3, 1021, 1, 102, 4, 0.4, 2624, 1, 0.2, 388, 3, 2, 2,3.15580068,0.5845534407027818,0.68021723,142
2,9994,1,50,1,0.4,2294,5,0.4,271,3,3,2,3.0524485346713806,0.5885797950219619,0.43551749026165437,274
1,8276,1,50,4,0.3,2523,10,0.3,301,2,2,1,3.0006108740326600248,0.6054172767203514,0.4883522980807509,298
2,6890,2,40,2,0.5,2674,10,0.5,511,1,2,2,3.00745135915500672,0.5885797950219619,0.9242367999701785,312
2,2725,2,45,4,0.4,2707,1,0.4,264,3,1,1,3.03200355346167605,0.5867496339677891,0.4344057401098985,284
2,2725,2,45,4,0.4,2707,1,0.4,264,3,1,1,3.03200355346167605,0.5867496339677891,0.4344057401098985,284
2,2714,4,61,4,0.3,1151,1,0.3,256,2,1,2,3.006372502885193754,0.5757686676427526,0.4485429578378386,210
2,2193,2,60,4,0.4,2610,6,0.4,473,3,2,1,0.015127742797198973,0.582723279648609,0.8356153578550143,331
2,2725,2,45,4,0.4,2707,1,0.4,264,3,1,1,0.03200355346167605,0.5867496339677891,0.4344057401098985,284
2,4264,1,65,4,0.4,2875,1,0.4,260,3,1,2,0.004035252785190124,0.5845534407027818,0.41555627945475165,260