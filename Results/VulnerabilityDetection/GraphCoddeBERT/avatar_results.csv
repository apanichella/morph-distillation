Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
2,17313,3,40,4,0.5,60,2,0.5,479,1,2,1,3.000088,0.5933382137628112,0.841916745,423
1,2865,1,168,1,0.5,121,6,0.5,328,1,2,2,3.00306,0.5885797950219619,0.55300472,459
2,1191,5,102,2,0.3,357,3,0.3,260,1,3,3,3.007516,0.5757686676427526,0.53560052,351
2,3246,5,96,3,0.2,196,8,0.2,432,3,2,3,3.000104,0.5922401171303074,1.077033456,423
2,7398,1,90,2,0.2,58,10,0.2,265,3,2,1,3.00084,0.5988286969253295,0.426158955,423
4,3625,1,144,1,0.2,122,4,0.2,463,3,1,1,3.002608,0.58199121522694,0.809549481,367
2,11190,2,44,1,0.3,1239,11,0.3,363,2,3,3,3.002656,0.592606149341142,0.624602484,483
2,2202,1,192,2,0.5,78,8,0.5,379,3,2,3,3.00064,0.5856515373352855,0.663760513,358
4,3703,6,60,4,0.5,554,1,0.5,470,3,1,2,3.000616,0.5772327964860908,1.04455902,351
3,2145,8,96,1,0.5,115,4,0.5,468,1,3,2,3.002648,0.6057833089311859,1.440152064,438