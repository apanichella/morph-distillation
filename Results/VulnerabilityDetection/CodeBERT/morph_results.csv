Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
2,1442,10,48,2,0.2,606,3,0.2,289,3,3,1,2.9785426184517103,0.5966325036603221,0.6203984787410126,323
4,1275,11,48,2,0.2,638,8,0.2,302,2,3,1,3.5855687805197287,0.58601756954612,0.8373395814386317,269
2,1915,4,27,3,0.2,2824,3,0.2,485,3,3,1,3.086817942419254,0.5940702781844802,0.9362887466595902,322
2,1415,4,50,2,0.2,1503,1,0.2,368,1,3,1,3.026596146917961,0.5948023426061494,0.6869235592684354,343
2,45330,2,16,2,0.2,204,1,0.2,360,2,1,1,2.9746211079040137,0.6002928257686676,0.5513986094970055,329
2,1348,4,27,1,0.3,2806,9,0.3,493,2,2,1,2.935049720131005,0.58601756954612,0.973507122783809,239
3,10782,1,65,3,0.3,258,5,0.3,445,2,2,1,3.1605067188915053,0.6156661786237189,0.762204240045183,357
3,41193,1,18,2,0.2,58,6,0.2,260,3,3,3,3.009039730934267,0.6068814055636896,0.404607583353905,354
2,3022,2,127,3,0.2,309,1,0.2,363,1,3,1,3.006862627523506,0.5907759882869692,0.6720037143825278,346
2,28901,11,21,2,0.4,258,7,0.4,284,1,1,2,3.0315561116696887,0.5878477306002928,0.5744038081338916,362
2,3022,2,127,3,0.2,309,1,0.2,363,1,3,1,3.006862627523506,0.5907759882869692,0.6720037143825278,346
2,28576,11,20,2,0.4,332,10,0.4,270,2,1,3,2.9619963840036605,0.6046852122986823,0.5446584829041857,339