Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
4,3509,1,132,4,0.5,511,11,0.5,375,2,3,1,3.019044,0.5907759882869692,0.64495425,433
4,27536,1,24,4,0.2,1516,6,0.2,467,1,2,1,3.000792,0.599194729136164,0.739326847,454
1,5457,5,84,2,0.3,112,2,0.3,441,2,1,1,3.001304,0.6032210834553441,1.003866381,451
2,36945,1,20,3,0.4,60,10,0.4,288,1,3,1,3.000632,0.6039531478770132,0.44683344,435
4,3011,4,110,3,0.5,184,5,0.5,300,2,3,3,3.002408,0.5922401171303074,0.6211212,430
2,1184,2,171,3,0.6,252,9,0.6,458,1,3,1,3.001364,0.5922401171303074,0.988427204,443
1,3211,2,130,2,0.2,190,5,0.2,464,3,2,2,3.003272,0.599194729136164,0.92743856,445
4,27536,1,24,4,0.2,1516,6,0.2,467,1,2,1,3.000792,0.599194729136164,0.739326847,454
2,3714,8,80,4,0.4,160,5,0.4,282,3,2,2,3.001272,0.6035871156661786,0.65786088,449
1,1757,1,189,3,0.5,281,9,0.5,496,1,2,3,3.001328,0.5940702781844802,0.91902848,438