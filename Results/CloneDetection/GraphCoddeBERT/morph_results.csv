Tokenizer,Vocab Size,Num Hidden Layers,Hidden Size,Hidden Act,Hidden Dropout Prob,Intermediate Size,Num Attention Heads,Attention Probs Dropout Prob,Max Sequence Length,Position Embedding Type,Learning Rate,Batch Size,Size,Accuracy,FLOPS,Flips
4,1002,3,176,4,0.2,40,4,0.2,468,3,2,1,3.0330394668707923,0.982,3.02043353668501,24
1,1001,2,66,3,0.4,3030,6,0.4,335,2,1,1,3.0076396440238593,0.935,1.2428213702742938,38
1,6059,2,81,3,0.5,575,9,0.5,336,3,1,1,2.9795078002977724,0.94525,1.286298386042553,54
1,3185,1,146,3,0.2,33,1,0.2,465,3,2,1,2.984280117714487,0.9585,1.8927308425427012,53
3,4356,1,126,2,0.3,36,9,0.3,383,3,3,1,3.0488667701301173,0.9625,1.5443299636978929,24
4,2217,12,69,2,0.3,206,1,0.3,258,2,2,1,3.018720005845905,0.952,1.6579056691365364,49
2,1070,1,220,4,0.3,45,1,0.3,401,3,1,2,2.9560800558728193,0.85025,1.7215024823800715,58
1,41347,3,18,4,0.3,107,9,0.3,432,3,1,2,3.0993313038984525,0.9475,1.6584697985897014,65
1,3185,1,146,3,0.2,33,1,0.2,465,3,2,1,2.984280117714487,0.9585,1.8927308425427012,53
1,2094,1,81,4,0.5,2990,9,0.5,367,3,1,2,3.0874209081635997,0.94075,1.3708480980806086,63
4,1445,6,80,1,0.3,399,1,0.3,483,3,2,1,3.001616,0.969,3.301880736,27
4,34499,1,18,2,0.3,2534,9,0.3,400,3,1,2,2.937915343924994,0.91625,1.2966193929546732,55
1,5230,1,94,3,0.2,609,1,0.2,475,3,2,1,3.018573550344539,0.95375,1.78029422279025,53
1,1034,1,234,3,0.2,49,9,0.2,321,2,2,2,2.991128934479586,0.9505,1.3145193347611246,56
4,34268,1,18,4,0.2,3028,9,0.2,267,2,1,2,3.028663837823266,0.93525,0.8753948745324786,51
2,15053,1,5,3,0.4,422,1,0.2,256,2,1,1,3.00588680736,0.9112,0.844928842,28